import torch
import torch.nn as nn
import torch.optim as optim
from model import GPTBackbone
from model.utils import count_parameters
from pathlib import Path
import csv
from datetime import datetime


from utils.tokenizer import load_tokenizer

class TransformerTrainer:
    def __init__(self, config):
        self.dataset_path = Path("data/train.txt")
        self.source = config.get("dataset_source", "local")  # 'local' or 'web'
        self.lines = self._load_text_dataset()
        self.config = config
        self.device = config["device"]
        self.tokenizer = load_tokenizer()
        self.model = self._build_model()
        self.criterion = nn.CrossEntropyLoss()
        self.optimizer = optim.AdamW(self.model.parameters(), lr=config["lr"])

        self.project_root = Path(__file__).resolve().parents[1]
        self.ckpt_dir = self.project_root / config["ckpt_dir"]
        self.log_dir = self.project_root / config["log_dir"]
        self.ckpt_dir.mkdir(parents=True, exist_ok=True)
        self.log_dir.mkdir(parents=True, exist_ok=True)
        self.log_path = self.log_dir / "train_log.csv"

    def _load_text_dataset(self):
        if self.source == "local":
            if not self.dataset_path.exists():
                raise FileNotFoundError(f"Dataset not found at {self.dataset_path}")
            with self.dataset_path.open("r", encoding="utf-8") as f:
                return [line.strip() for line in f if line.strip()]
        elif self.source == "web":
            print("[!] Web crawling is not implemented yet — returning fallback data.")
            return ["Generated by web placeholder"] * 1000
        else:
            raise ValueError(f"Unknown dataset source: {self.source}")

    def _build_model(self):
        model = GPTBackbone(
            vocab_size=self.config["vocab_size"],
            max_len=self.config["max_len"],
            embed_dim=self.config["embed_dim"],
            num_heads=self.config["num_heads"],
            num_layers=self.config["num_layers"]
        )
        print(f"Model initialized with {count_parameters(model):,} parameters.")
        return model.to(self.device)

    def _generate_dummy_batch(self):
        start_idx = torch.randint(0, max(1, len(self.lines) - self.config["batch_size"]), (1,)).item()
        batch_texts = self.lines[start_idx : start_idx + self.config["batch_size"]]
        encodings = self.tokenizer(
            batch_texts,
            return_tensors="pt",
            padding="max_length",
            max_length=self.config["max_len"],
            truncation=True
        )
        return encodings["input_ids"].to(self.device)

    def train_step(self, input_ids, labels):
        self.model.train()
        logits = self.model(input_ids)
        loss = self.criterion(logits.view(-1, logits.size(-1)), labels.view(-1))
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()
        return loss.item()

    def save_checkpoint(self, epoch, loss):
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        ckpt_path = self.ckpt_dir / f"checkpoint_epoch_{epoch:03d}_{timestamp}.pt"
        torch.save({
            "epoch": epoch,
            "model_state_dict": self.model.state_dict(),
            "optimizer_state_dict": self.optimizer.state_dict(),
            "loss": loss,
            "config": self.config
        }, ckpt_path)
        print(f"✅ Checkpoint saved: {ckpt_path}")

    def log_metrics(self, epoch, train_loss, eval_loss, perplexity):
        write_header = not self.log_path.exists()
        with self.log_path.open(mode="a", newline="") as f:
            writer = csv.writer(f)
            if write_header:
                writer.writerow(["timestamp", "epoch", "train_loss", "eval_loss", "perplexity"] + list(self.config.keys()))
            writer.writerow([datetime.now().isoformat(), epoch, train_loss, eval_loss, perplexity] + list(self.config.values()))

    @torch.no_grad()
    def evaluate(self, num_batches=5):
        self.model.eval()
        total_loss = 0
        for _ in range(num_batches):
            input_ids = self._generate_dummy_batch()
            logits = self.model(input_ids)
            loss = self.criterion(logits.view(-1, logits.size(-1)), input_ids.view(-1))
            total_loss += loss.item()
        avg_loss = total_loss / num_batches
        perplexity = torch.exp(torch.tensor(avg_loss)).item()
        print(f"Validation Loss: {avg_loss:.4f} | Perplexity: {perplexity:.2f}")
        return avg_loss, perplexity

    def train(self):
        for epoch in range(1, self.config["epochs"] + 1):
            input_ids = self._generate_dummy_batch()
            train_loss = self.train_step(input_ids, input_ids.clone())
            eval_loss, perplexity = self.evaluate()
            print(f"Epoch {epoch}/{self.config['epochs']} - Train Loss: {train_loss:.4f} - Eval Loss: {eval_loss:.4f} - Perplexity: {perplexity:.2f}")
            self.save_checkpoint(epoch, train_loss)
            self.log_metrics(epoch, train_loss, eval_loss, perplexity)


def get_config():
    # Change 'dataset_source' to 'web' to experiment with crawling later
    return {
        "vocab_size": 1000,
        "max_len": 32,
        "batch_size": 4,
        "embed_dim": 128,
        "num_heads": 4,
        "num_layers": 2,
        "lr": 1e-4,
        "epochs": 5,
        "log_dir": "logs",
        "ckpt_dir": "checkpoints",
        "device": "cuda" if torch.cuda.is_available() else "cpu",
        "dataset_source": "local"  # Set it to "web" to later support internet crawling
    }


if __name__ == "__main__":
    config = get_config()
    trainer = TransformerTrainer(config)
    trainer.train()
