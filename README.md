# ğŸ§  Multi-Modular LLM â€” Transformer Training Script

This project implements a clean, modular pipeline to train a GPT-style transformer model using PyTorch. It includes causal masking, real tokenization (via Hugging Face), logging, checkpointing, and future support for crawling text from the web.

---

## ğŸ“ Project Structure

.
â”œâ”€â”€ data/
â”‚   â””â”€â”€ train.txt                  # Text training data (one sentence per line)
â”œâ”€â”€ logs/
â”‚   â””â”€â”€ train_log.csv              # Epoch metrics with config and perplexity
â”œâ”€â”€ checkpoints/
â”‚   â””â”€â”€ checkpoint_epoch_*.pt     # Saved model weights
â”œâ”€â”€ model/
â”‚   â””â”€â”€ transformer.py            # GPT backbone with causal masking
â”œâ”€â”€ utils/
â”‚   â””â”€â”€ tokenizer.py              # Loads Hugging Face tokenizer
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ generate_text_dataset.py  # Generates local dataset for testing
â”œâ”€â”€ training/
â”‚   â””â”€â”€ train_transformer.py      # Full training loop
â””â”€â”€ README.md                     # You are here

---

## ğŸ§ª Training Workflow

1. Generate data:
   python scripts/generate_text_dataset.py

2. Train the model:
   python training/train_transformer.py

3. Check logs and checkpoints:
   - logs/train_log.csv
   - checkpoints/checkpoint_epoch_*.pt

---

## âš™ï¸ Configuration

Defined in `get_config()` in `train_transformer.py`:

- vocab_size, max_len, embed_dim, num_heads, num_layers
- batch_size, lr, epochs
- dataset_source: \"local\" or \"web\" (placeholder for now)


